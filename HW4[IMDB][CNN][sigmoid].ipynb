{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB & CNN\n",
    "# *Dataset에 대해 추가 조사 필요\n",
    "# - 실제 데이터에 대한 상세 기술\n",
    "# - 예측하고자 하는 방법/방향 기술 : batch size를 64, hidden dimension을 250으로 CNN을 통해 테스트해서 accuracy를 높이고자 했다.\n",
    "# - 학습을 위해 데이터가 어떻게 가공/처리되었는지 기술\n",
    "# - 사용된 모델의 특징에 대한 기술\n",
    "# - 생성된 모델의 model.summary() 출력\n",
    "# - epoch은 최소 25회 이상 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Activation\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_features = 5000\n",
    "MAX_len = 400\n",
    "Batch_size = 64\n",
    "Embedding_dimension = 120\n",
    "Filters = 250\n",
    "Kernel_size = 3\n",
    "Hidden_dimension = 250\n",
    "Epoch = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_len)\n",
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_features,\n",
    "                    Embedding_dimension,\n",
    "                    input_length=MAX_len))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv1D는 간단한 데이터 학습 예측에 적합한 컨볼루션 레이어이다.\n",
    "# 특정 부분만 추출 가능(전체 불가능)\n",
    "model.add(Conv1D(Filters,\n",
    "                 Kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 큰 벡터값만 추출해주는 풀링 함수이다.\n",
    "model.add(GlobalMaxPooling1D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 히든 레이어는 입력층과 결과층 사이에 존재하는 학습에 중대한 영향을 끼치는 계층이다.\n",
    "model.add(Dense(Hidden_dimension))\n",
    "\n",
    "# Dropout -> 네트워크의 일부를 생략 -> 생략한 네트워크는 학습에 영향을 안끼침\n",
    "# 특정 뉴런의 bias나 weight가 큰 값을 가지면\n",
    "# 이 뉴련의 영향이 커지게 되면서 다른 뉴런들의 학습 속도가 느려지거나 악영향을 끼칠 수 있다.\n",
    "# 이런 특정 뉴런의 영향을 받지 않기 떄문에 co-adaptation이 되는 것을 방지 할 수 있다.\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# sigmoid 함수는 backpropagation 수행할 때 각 계층들을 지나면서 gradient를 계속 곱하게\n",
    "# 되어서 gradient는 0으로 수렴한다. 그래서 계층이 많아지면 동작하지 않을 수 있다.\n",
    "# relu 함수는 input value가 0보다 작으면 0 / 0보다 크면 입력값을 보존한다.\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=Batch_size,\n",
    "          epochs=Epoch,\n",
    "          validation_data=(x_test, y_test))\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy : %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='val_loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
